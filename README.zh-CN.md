# Chat-RAG 🚀

<div align="center">

[![Go版本](https://img.shields.io/badge/Go-1.24.2-blue.svg)](https://golang.org/doc/go1.24) [![许可证](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE) [![Docker](https://img.shields.io/badge/docker-available-blue.svg)](Dockerfile) [![构建状态](https://img.shields.io/badge/build-passing-brightgreen.svg)](#)

[English](./README.md) | [中文](#chinese)

</div>

## 🎯 项目概述

Chat-RAG 是一个高性能、企业级的聊天服务，结合了大语言模型（LLM）与检索增强生成（RAG）功能。它为现代 AI 应用提供智能上下文处理、工具集成和流式响应功能。

### 核心特性

- **🧠 智能上下文处理**：先进的提示工程，支持上下文压缩和过滤
- **🔧 工具集成**：无缝集成语义搜索、代码定义查询和知识库查询
- **⚡ 流式支持**：通过服务器发送事件（SSE）实现实时流式响应
- **🛡️ 企业安全**：基于 JWT 的身份验证和请求验证
- **📊 全面监控**：内置指标和日志记录，支持 Prometheus
- **🔄 多模态支持**：支持各种 LLM 模型和函数调用
- **🚀 高性能**：优化的低延迟响应和高吞吐量
 - **🤖 语义路由（来自 ai-llm-router 迁移）**：可选开启，自动按语义选择下游模型；在响应头透出 `x-select-llm`、`x-user-input`

## 🏗️ 架构设计

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   API 网关      │───▶│  聊天处理器     │───▶│  提示引擎       │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                       │                       │
         ▼                       ▼                       ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   身份验证      │    │  LLM 客户端     │    │  工具执行器     │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                       │                       │
         ▼                       ▼                       ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   指标监控      │    │  Redis 缓存     │    │  搜索工具       │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

## 🚀 快速开始

### 环境要求

- Go 1.24.2 或更高版本
- Redis 6.0+（可选，用于缓存）
- Docker（可选，用于容器化部署）

### 安装步骤

```bash
# 克隆仓库
git clone https://github.com/zgsm-ai/chat-rag.git
cd chat-rag

# 安装依赖
make deps

# 构建应用
make build

# 使用默认配置运行
make run
```

### Docker 部署

```bash
# 构建 Docker 镜像
make docker-build

# 运行容器
make docker-run
```

## ⚙️ 配置说明

服务通过 YAML 文件进行配置。查看 [`etc/chat-api.yaml`](etc/chat-api.yaml) 了解默认配置：

```yaml
# 服务
Host: 0.0.0.0
Port: 8080

# LLM 上游（单一端点；具体模型由请求体的 model 字段决定）
LLM:
  Endpoint: "http://localhost:8000/v1/chat/completions"
  # 可选：支持函数调用的模型清单
  FuncCallingModels: ["gpt-4o-mini", "o4-mini"]

# 上下文压缩
ContextCompressConfig:
  EnableCompress: true
  TokenThreshold: 5000
  SummaryModel: "deepseek-v3"
  SummaryModelTokenThreshold: 4000
  RecentUserMsgUsedNums: 4

# 工具（RAG 后端）
Tools:
  SemanticSearch:
    SearchEndpoint: "http://localhost:8002/codebase-indexer/api/v1/semantics"
    ApiReadyEndpoint: "http://localhost:8002/healthz"
    TopK: 5
    ScoreThreshold: 0.3
  DefinitionSearch:
    SearchEndpoint: "http://localhost:8002/codebase-indexer/api/v1/definitions"
    ApiReadyEndpoint: "http://localhost:8002/healthz"
  ReferenceSearch:
    SearchEndpoint: "http://localhost:8002/codebase-indexer/api/v1/references"
    ApiReadyEndpoint: "http://localhost:8002/healthz"
  KnowledgeSearch:
    SearchEndpoint: "http://localhost:8003/knowledge/api/v1/search"
    ApiReadyEndpoint: "http://localhost:8003/healthz"
    TopK: 5
    ScoreThreshold: 0.3

# 日志与分类
Log:
  LogFilePath: "logs/chat-rag.log"
  LokiEndpoint: "http://localhost:3100/loki/api/v1/push"
  LogScanIntervalSec: 60
  ClassifyModel: "deepseek-v3"
  EnableClassification: true

# Redis（可选）
Redis:
  Addr: "127.0.0.1:6379"
  Password: ""
  DB: 0

# 语义路由（从 ai-llm-router 迁移而来，需将请求体 model 置为 "auto" 才触发）
router:
  enabled: true
  strategy: semantic
  semantic:
    analyzer:
      model: gpt-4o-mini
      timeoutMs: 3000
      # 可选高级项：
      # totalTimeoutMs: 5000
      # maxInputBytes: 8192
      # promptTemplate: ""   # 自定义分类 Prompt，不配置则使用内置默认
      # analysisLabels: ["simple_request", "planning_request", "code_modification"]
      # dynamicMetrics:
      #   enabled: false
      #   redisPrefix: "ai_router:metrics:"
      #   metrics: ["error_rate", "p99", "circuit"]
    inputExtraction:
      protocol: openai
      userJoinSep: "\n\n"
      stripCodeFences: true
      codeFenceRegex: ""
      maxUserMessages: 100
      maxHistoryBytes: 4096
    routing:
      candidates:
        - modelName: "gpt-4o-mini"
          enabled: true
          scores:
            simple_request: 10
            planning_request: 5
            code_modification: 3
        - modelName: "o4-mini"
          enabled: true
          scores:
            simple_request: 4
            planning_request: 8
            code_modification: 6
      minScore: 0
      tieBreakOrder: ["o4-mini", "gpt-4o-mini"]
      fallbackModelName: "gpt-4o-mini"
    ruleEngine:
      enabled: false
      inlineRules: []
      bodyPrefix: "body."
      headerPrefix: "header."
```

#### 配置字段详解（节选）

- LLM
  - Endpoint：统一的 Chat Completions 端点；最终模型名通过请求体 `model` 传递
  - FuncCallingModels：具备函数调用能力的模型清单，便于按需启用工具
- ContextCompressConfig
  - EnableCompress：是否开启长上下文压缩
  - TokenThreshold：超过此阈值触发压缩
  - SummaryModel / SummaryModelTokenThreshold：用于摘要压缩的模型与阈值
  - RecentUserMsgUsedNums：压缩流程中参照的最近用户消息数量
- Tools（RAG）
  - 各搜索模块提供 HTTP 端点；TopK/ScoreThreshold 控制召回数量与质量
- Log
  - LogFilePath：本地日志文件路径；后台进程会批量上传至 Loki
  - LokiEndpoint：Loki Push 端点
  - LogScanIntervalSec：日志扫描与上传周期
  - ClassifyModel / EnableClassification：是否使用 LLM 对日志分类
- Redis：可选；用于工具状态、路由动态指标等
- router（语义路由）
  - enabled/strategy：开启语义路由；当前策略为 `semantic`
  - semantic.analyzer：分类所用模型与超时；可自定义 Prompt 与标签；可选动态指标（Redis）
  - semantic.inputExtraction：控制用户输入与历史的抽取方式，支持去除代码块、限制历史长度
  - semantic.routing：候选模型评分表；通过 `tieBreakOrder` 解决同分，`fallbackModelName` 兜底
  - semantic.ruleEngine：可选的规则引擎预筛模型，默认关闭

## 📡 API 端点

### 聊天完成（非流式）

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [
      {"role": "user", "content": "今天天气怎么样？"}
    ],
    "stream": false
  }'
```

### 启用语义路由（自动选型）

将请求体中的 `model` 置为 `auto`，并在配置中开启 `router.enabled: true`：

```bash
curl -i -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "auto",
    "messages": [
      {"role": "user", "content": "给我一个详细的改造方案并产出代码示例"}
    ],
    "stream": false
  }'
```

响应头将包含：
- `x-select-llm`：最终选择的下游模型名
- `x-user-input`：用于分类的用户输入（已清洗并进行 base64 编码）

### 流式响应

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [
      {"role": "user", "content": "写一个 Python 函数"}
    ],
    "stream": true
  }'
```

### 指标监控

Prometheus 指标暴露在 `/metrics`，详见 `METRICS.md`。

## 🔧 开发指南

### 项目结构

```
chat-rag/
├── internal/
│   ├── handler/          # HTTP 处理器
│   ├── logic/           # 业务逻辑
│   ├── router/          # 语义路由（策略 + 工厂）
│   ├── client/          # 外部服务客户端
│   ├── promptflow/      # 提示处理管道
│   ├── functions/       # 工具执行引擎
│   └── config/          # 配置管理
├── etc/                 # 配置文件
├── test/               # 测试文件
└── deploy/             # 部署配置
```

### 可用命令

```bash
make help              # 显示可用命令
make build            # 构建应用
make test             # 运行测试
make fmt              # 格式化代码
make vet              # 检查代码
make docker-build     # 构建 Docker 镜像
make dev              # 运行开发服务器（支持热重载）
```

### 测试

```bash
# 运行所有测试
make test

# 运行特定测试
go test -v ./internal/logic/

# 带覆盖率运行
go test -cover ./...
```

## 🔍 高级功能

### 上下文压缩

智能上下文压缩处理长对话：

```yaml
ContextCompressConfig:
  EnableCompress: true
  TokenThreshold: 5000
  SummaryModel: "deepseek-v3"
  SummaryModelTokenThreshold: 4000
  RecentUserMsgUsedNums: 4
```

### 工具集成

支持多种搜索和分析工具：

- **语义搜索**：基于向量的代码和文档搜索
- **定义搜索**：代码定义查询
- **引用搜索**：代码引用分析
- **知识搜索**：文档知识库查询

### 语义路由（来自 ai-llm-router 迁移）

当 `router.enabled: true` 且请求体 `model` 为 `auto` 时，将自动选择最合适的下游模型：

1. 输入抽取：按 `router.semantic.inputExtraction` 提取当前输入与少量历史，可选移除代码块
2. 语义分类：调用 `router.semantic.analyzer.model` 获取标签（默认：simple_request / planning_request / code_modification）
3. 候选打分：在 `routing.candidates` 中按标签取分；支持 `minScore` 和动态指标（可选）
4. Tie-break 与回退：用 `tieBreakOrder` 破同分；失败或低于阈值则使用 `fallbackModelName`
5. 可观测性：在响应头写入 `x-select-llm` 与 `x-user-input`（后者做过清洗并 base64 编码）

### 基于代理的处理

可配置的代理匹配，用于专门任务：

```yaml
AgentsMatch:
  - AgentName: "strict"
    MatchKey: "a strict strategic workflow controller"
  - AgentName: "code"
    MatchKey: "a highly skilled software engineer"
```

## 📊 监控和可观测性

### 指标监控

服务在 `/metrics` 端点暴露 Prometheus 指标：

- 请求计数和延迟
- Token 使用统计
- 工具执行指标
- 错误率和类型

### 日志记录

使用 Zap 记录器进行结构化日志记录：

- 请求/响应日志记录
- 错误跟踪
- 性能指标
- 调试信息

## 🔒 安全特性

- 基于 JWT 的身份验证
- 请求验证和清理
- 速率限制支持
- 安全头部处理

## 🚢 部署方案

### 生产部署

```bash
# 构建生产版本
CGO_ENABLED=0 GOOS=linux go build -o chat-rag .

# 使用生产配置运行
./chat-rag -f etc/prod.yaml
```

### Kubernetes 部署

查看 [`deploy/`](deploy/) 目录中的 Kubernetes 清单和 Helm 图表。

## 🤝 贡献指南

1. Fork 仓库
2. 创建功能分支（`git checkout -b feature/amazing-feature`）
3. 提交更改（`git commit -m 'Add some amazing feature'`）
4. 推送到分支（`git push origin feature/amazing-feature`）
5. 打开拉取请求

## 📄 许可证

本项目采用 MIT 许可证 - 查看 [LICENSE](LICENSE) 文件了解详情。

## 🆘 支持

如需支持和提问：
- 在 GitHub 仓库中创建问题
- 联系维护者

---

<div align="center">
  <b>⭐ 如果这个项目对你有帮助，请给我们一个星标！</b>
</div>